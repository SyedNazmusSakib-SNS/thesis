<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   References
  </title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet"/>
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700;800&amp;family=Roboto:wght@300;400;500;700&amp;display=swap" rel="stylesheet"/>
  <link href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" rel="stylesheet"/>
  <style>
   body {
            margin: 0;
            padding: 0;
            background-color: #f3f4f6;
            font-family: 'Roboto', sans-serif;
            overflow: hidden;
        }
        .slide-container {
            width: 1280px;
            height: 720px;
            background-color: #ffffff;
            position: relative;
            overflow: hidden;
            display: flex;
            flex-direction: column;
        }
        h1, h2, h3, h4, h5, h6 {
            font-family: 'Montserrat', sans-serif;
        }
        .header-bg {
            background-color: #0f172a; /* Navy Blue */
        }
        .ref-item {
            break-inside: avoid;
            margin-bottom: 0.5rem;
        }
  </style>
 </head>
 <body>
  <div class="slide-container">
   <!-- Header -->
   <div class="header-bg w-full h-20 px-14 flex items-center justify-between shadow-md z-20 flex-shrink-0">
    <div class="flex items-center space-x-4">
     <div class="w-12 h-1 bg-teal-500 rounded-full">
     </div>
     <h1 class="text-3xl font-bold text-white tracking-wide">
      References
     </h1>
    </div>
    <div class="text-gray-400 text-sm font-medium">
     References | Slide 25
    </div>
   </div>
   
   <!-- Main Content -->
   <div class="flex-1 w-full h-full p-12 pt-8 bg-white relative z-10 overflow-hidden">
    <div class="columns-2 gap-12 text-xs text-gray-600 leading-snug">
     
     <!-- [1] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[1]</span>
      <p>M. M. Monshi, J. Poon, V. Chung, <strong>Deep learning in generating radiology reports: A survey</strong>, <em>Artificial Intelligence in Medicine 106</em> (2020) 101878.</p>
     </div>

     <!-- [2] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[2]</span>
      <p>Z. Chen, M. Varma, J.-B. Delbrouck, et al., <strong>Chexagent: Towards a foundation model for chest x-ray interpretation</strong>, <em>arXiv preprint arXiv:2401.12208</em> (2024).</p>
     </div>

     <!-- [3] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[3]</span>
      <p>A. E. Johnson, et al., <strong>Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports</strong>, <em>Scientific Data 6 (1)</em> (2019) 317.</p>
     </div>

     <!-- [4] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[4]</span>
      <p>J. T. Wu, N. N. Agu, I. Lourentzou, et al., <strong>Chest imagenome dataset for clinical reasoning</strong>, <em>arXiv preprint arXiv:2108.00316</em> (2021).</p>
     </div>

     <!-- [5] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[5]</span>
      <p>S. Jain, et al., <strong>Radgraph: Extracting clinical entities and relations from radiology reports</strong>, <em>NeurIPS Datasets and Benchmarks Track</em>, 2021.</p>
     </div>

     <!-- [6] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[6]</span>
      <p>Z. Chen, et al., <strong>Chexagent: Towards a foundation model for chest x-ray interpretation</strong>, <em>arXiv preprint arXiv:2401.12208</em> (2024).</p>
     </div>

     <!-- [7] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[7]</span>
      <p>F. Perez-Garcia, et al., <strong>Llava-rad: Training a large language-and-vision assistant for biomedicine in one day</strong>, <em>arXiv preprint arXiv:2306.00890</em> (2024). (Updated version).</p>
     </div>

     <!-- [8] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[8]</span>
      <p>S. Bannur, et al., <strong>Maira-2: Grounded radiology report generation</strong>, <em>arXiv preprint arXiv:2406.04449</em> (2024).</p>
     </div>

     <!-- [9] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[9]</span>
      <p>J. T. Wu, et al., <strong>Chest imagenome dataset for clinical reasoning</strong>, <em>NeurIPS</em> (2021).</p>
     </div>

     <!-- [10] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[10]</span>
      <p>X. Zhang, Z. Meng, J. Lever, E. S. Ho, <strong>Libra: Leveraging temporal images for biomedical radiology analysis</strong>, <em>ACL Findings</em>, 2025.</p>
     </div>

     <!-- [11] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[11]</span>
      <p>S. Ostmeier, et al., <strong>Green: Generative radiology report evaluation and error notation</strong>, <em>arXiv preprint arXiv:2405.03595</em> (2024).</p>
     </div>

     <!-- [12] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[12]</span>
      <p>J. Wei, et al., <strong>Chain-of-thought prompting elicits reasoning in large language models</strong>, <em>NeurIPS Vol. 35</em>, 2022.</p>
     </div>

     <!-- [13] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[13]</span>
      <p>S. Yao, et al., <strong>React: Synergizing reasoning and acting in language models</strong>, <em>ICLR</em>, 2023.</p>
     </div>

     <!-- [14] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[14]</span>
      <p>A. Madaan, et al., <strong>Self-refine: Iterative refinement with self-feedback</strong>, <em>NeurIPS</em>, 2023.</p>
     </div>

     <!-- [15] -->
     <div class="ref-item flex">
      <span class="font-bold text-teal-600 mr-2 min-w-[1.5rem]">[15]</span>
      <p>J. M. Chaves, V. M. Rao, et al., <strong>Rexerr: Synthesizing clinically meaningful errors in diagnostic radiology reports</strong>, <em>arXiv preprint arXiv:2409.10829</em> (2024).</p>
     </div>

    </div>
   </div>

   <!-- Footer Decoration -->
   <div class="absolute bottom-0 w-full h-3 flex z-20">
    <div class="h-full w-1/3 bg-blue-900">
    </div>
    <div class="h-full w-1/3 bg-teal-600">
    </div>
    <div class="h-full w-1/3 bg-blue-800">
    </div>
   </div>
  </div>
 </body>
</html>
